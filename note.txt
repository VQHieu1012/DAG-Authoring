#### Airflow level ####

# the max number of concurrency tasks your airflow instance can run at the same time
# it means your airflow instance can run at most 32 tasks at the same time
parallelism = 32 


# the max number of tasks your dag can run at the same time
# you won't have more than 16 tasks run for all dagrun for your dag
dag_concurrency = 16


# the number of dagruns that can run at the same time for a given dag
max_active_run_per_dag = 16


#### DAG level ####

# at most 2 tasks run at the same time across all of the dagruns of the given dag [concurrency]
# 1 dagrun at the time for this specific dag [max_active_runs]
with (..., concurrency=2, max_active_runs=1)

#### Task level ####

# only 1 instance of task at the time across all dagruns of your dag [task_concurency]
PythonOperator(task_id="", task_concurency=1, pool='default_pool')

# pool_slots defined the slot the task will take in the pool while task is running
PythonOperator(task_id="", task_concurency=1, pool='default_pool', pool_slots=3)

# In tag, we have "priority_weight", but this only takes effect if these tasks share the same pool

# Keyword for task: depends_on_past=True
for example:
dagrun #1: t1 >> t2 >> t3
dagrun #2: t1 >> t2 >> t3

If t1_#1 fail, t1_#2 will not trigger and won't have any state -> you need to set timeout for task

# Keyword for task: wait_for_downstream=True (this will set depends_on_past=True automatically)
for example: 
t1_#1 is succeed and t2_#1 is directly downstream of the t1_#1 and succeed, 
t1_#2 will be triggered

In case you need to run multiple dagrun and access same resources, you may come up with race condition and so on,
So in order to ensure that your dag are not modifying the same resources at the same time, you can consider using this.


# Combo keywords for Retry task
retries: number of time a task retries
retry_delay: delay time between each Retry
retry_exponential_backoff: Set to True and the time delay between 2 retries will increase
max_retry_delay: max delay time between 2 retries

